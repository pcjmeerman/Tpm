{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The importance of space\n",
    "Agent based models are useful when the aggregate system behavior emerges out of local interactions amongst the agents. In the model of the evolution of cooperation, we created a set of agents and let all agents play against all other agents. Basically, we pretended as if all our agents were perfectly mixed. In practice, however, it is much more common that agents only interact with some, but not all, other agents. For example, in models of epidemiology, social interactions are a key factors. Thus, interactions are dependend on your social network. In other situations, our behavior might be based on what we see around us. Phenomena like fashion are at least partly driven by seeing what others are doing and mimicking this behavior. The same is true for many animals. Flocking dynamics as exhibited by starling, or shoaling behavior in fish, can be explained by the animal looking at its neirest neighbors and staying within a given distance of them. In agent based models, anything that structures the interaction amongst agents is typically called a space. This space can be a 2d or 3d space with euclidian distances (as in models of flocking and shoaling), it can also be a grid structure (as we will show below), or it can be a network structure. \n",
    "\n",
    "MESA comes with several spaces that we can readily use. These are\n",
    "\n",
    "* **SingleGrid;** an 'excel-like' space with each agent occopying a single grid cell\n",
    "* **MultiGrid;** like grid, but with more than one agent per grid cell\n",
    "* **HexGrid;** like grid, but on a hexagonal grid (*e.g.*, the board game Catan) thus changing who your neighbours are\n",
    "* **ConinuousSpace;** a 2d continous space were agents can occupy any coordinate\n",
    "* **NetworkGrid;** a network structure were one or more agents occupy a given node.\n",
    "\n",
    "A key concern when using a none-networked space, is to think carefull about what happens at the edges of the space. In a basic implementation, agents in for example the top left corner has only 2 neighbors, while an agent in the middle has four neighbors. This can give rise to artifacts in the results. Basically, the dynamics at the edges are different from the behavior further away from the edges. It is therefore quite common to use a torus, or donut, shape for the space. In this way, there is no longer any edge and artifacts are thus removed.\n",
    "\n",
    "\n",
    "# The emergence of cooperation in space\n",
    "The documentation of MESA on the different spaces is quite limited. Therefore, this assignment is largely a tutorial continuing on the evolution of cooperation. \n",
    "\n",
    "We make the following changes to the model\n",
    "\n",
    "* Each agent gets a position, which is an x,y coordinate indicating the grid cell the agent occupies.\n",
    "* The model has a grid, with an agent of random class. We initialize the model with equal probabilities for each type of class.\n",
    "* All agents play against their neighbors. On a grid, neighborhood can be defined in various ways. For example, a Von Neumann neighborhood contains the four cells that share a border with the central cell. A Moore neighborhood with distance one contains 8 cells by also considering the diagonal. Below, we use a neighborhood distance of 1, and we do include diagonal neighbors. So we set Moore to True. Feel free to experiment with this model by setting it to False, \n",
    "* The evolutionary dynamic, after all agents having played, is that each agent compares its scores to its neighbors. It will adopt whichever strategy within its neighborhood performed best.\n",
    "* Next to using a SingleGrid from MESA, we also use a DataCollector to handle collecting statistics.\n",
    "\n",
    "Below, I discuss in more detail the code containing the most important modifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T14:11:33.280171Z",
     "start_time": "2020-10-09T14:11:32.178821Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque, Counter, defaultdict\n",
    "from enum import Enum\n",
    "from itertools import combinations\n",
    "from math import floor\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from mesa import Model, Agent\n",
    "from mesa.space import SingleGrid\n",
    "from mesa.datacollection import DataCollector\n",
    "\n",
    "#Geen idee wat dit precies doet; leuk.\n",
    "class Move(Enum):\n",
    "    COOPERATE = 1\n",
    "    DEFECT = 2\n",
    "\n",
    "\n",
    "class AxelrodAgent(Agent):\n",
    "    \"\"\"An Abstract class from which all strategies have to be derived\n",
    "    \n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    points : int\n",
    "    pos : tuple\n",
    "    \n",
    "    \"\"\"\n",
    "    #Initialiseert een agent met twee atributen, die positie pos, hetgeen een tulip met een x en y waarde dient te zijn tussen de 0 en 20 vlgnsm mij. en natuurlijk de andere atribuut is het aantal punten.\n",
    "    def __init__(self, unique_id, pos, model):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.points = 0\n",
    "        self.pos = pos\n",
    "\n",
    "#Afhankelijk welke strategie we willen komt hier een andere invulling te staan voor de subclass\n",
    "    def step(self):\n",
    "        '''\n",
    "        This function defines the move and any logic for deciding\n",
    "        on the move goes here.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Move.COOPERATE or Move.DEFECT\n",
    "        \n",
    "        '''\n",
    "        #Mocht er iets fout gaan dan een error. logisch toch.\n",
    "        raise NotImplemetedError\n",
    "\n",
    "#Recieve payoff is iets dat elke agent in het model moet kunnen krijgen dus daarom staat het hier. de attribuut wordt self.points wordt geupdate. Maar dat hangt natuurlijk af van de payoff, die kan ook 0 zijn. Je moet natuurlijk wel winnen en dat hangt af van de voorgaande methode step.\n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        '''receive payoff and the two moves resulting in this payoff\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        payoff : int\n",
    "        my_move : {Move.COOPERATE, Move.DEFECT}\n",
    "        opponements_move : {Move.COOPERATE, Move.DEFECT}\n",
    "        \n",
    "        '''\n",
    "        self.points += payoff\n",
    "        \n",
    "        #Deze functie; reset wordt pas ingezet als het hele spelletje voorbij is en alles weer op nul gezet wordt. Wat dat reset inhoud is soms anders en daarom is het gespecificeerd in de aparte agent sub classes.\n",
    "    def reset(self):\n",
    "        '''\n",
    "        This function is called after playing N iterations against\n",
    "        another player.\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "#Dit is de eerste agent strategie.\n",
    "class TitForTat(AxelrodAgent):\n",
    "    \"\"\"This class defines the following strategy: play nice, unless, \n",
    "    in the previous move, the other player betrayed you.\"\"\"\n",
    "    \n",
    "    def __init__(self, unique_id, pos, model):\n",
    "        super().__init__(unique_id, pos, model)\n",
    "        #Voordat het model gelopen heeft moet de tit voor tat al wel weten wat die de eerste keer moet doen. Voor de rest hang het altijd af van de opponent_last_move. Daarom zetten we die hier nu gewoon al neer. Daarna wordt die in de step method overwritten.\n",
    "        self.opponent_last_move = Move.COOPERATE\n",
    "    \n",
    "    def step(self):\n",
    "        #De agent wordt straks in een model gestopt en daar wordt opgeslagen wat de opponnent_last move was; die returned deze agent nu.\n",
    "        return self.opponent_last_move\n",
    "    \n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "\n",
    "        #Kijk hier is de recieve payoff beter gespecifieerd. De super() staat er voor dat een method in de bovenstaande klass wordt ingevuld,          namelijk de receiv_payoff moet namelijk weten wat de payoff moet zijn. Nu hangt die af van de my_move en de opponent_move:\n",
    "        # In het model zijn de payoffs gedefineert:\n",
    "        # payoff_a, payoff_b = self.payoff_matrix[(move_a, move_b)]\n",
    "        # agent_a.receive_payoff(payoff_a, move_a, move_b)\n",
    "        # agent_b.receive_payoff(payoff_b, move_b, move_a)\n",
    "\n",
    "        #Dit geeft een tulip waarde voor a en b, die heet payoff en wordt toegevoegd aan de betreffende agent; het is een beetje stom dat er hier verwezen moet worden voor de simpele operator =+ payoff, maar ja.\n",
    "\n",
    "        super().receive_payoff(payoff, my_move, opponent_move)\n",
    "        self.opponent_last_move = opponent_move\n",
    "        \n",
    "    def reset(self):\n",
    "        self.opponent_last_move = Move.COOPERATE\n",
    "\n",
    "\n",
    "class ContriteTitForTat(AxelrodAgent):\n",
    "    \"\"\"This class defines the following strategy:  play nice, unless, \n",
    "    in the previous two moves, the other player betrayed you.\"\"\" \n",
    "    \n",
    "    def __init__(self, unique_id, pos, model):\n",
    "        super().__init__(unique_id, pos, model)\n",
    "        self.opponent_last_two_moves = deque([Move.COOPERATE, Move.COOPERATE], maxlen=2)\n",
    "    \n",
    "    def step(self):\n",
    "        if (self.opponent_last_two_moves[0] == Move.DEFECT) and\\\n",
    "           (self.opponent_last_two_moves[1] == Move.DEFECT):\n",
    "            return Move.DEFECT\n",
    "        else:\n",
    "            return Move.COOPERATE\n",
    "    \n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        super().receive_payoff(payoff, my_move, opponent_move)\n",
    "        self.opponent_last_two_moves.append(opponent_move)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.opponent_last_two_moves = deque([Move.COOPERATE, Move.COOPERATE], maxlen=2)        \n",
    "\n",
    "\n",
    "#############################\n",
    "# Models from Assignment 1 #\n",
    "#############################\n",
    "\n",
    "class Defector(AxelrodAgent):\n",
    "    def step(self):\n",
    "        return Move.DEFECT\n",
    "        \n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "class Cooperator(AxelrodAgent):\n",
    "    def step(self):\n",
    "        return Move.COOPERATE\n",
    "    \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "class GrimTrigger(AxelrodAgent):\n",
    "    def __init__(self, unique_id, pos, model):\n",
    "        super().__init__(unique_id, pos, model)\n",
    "        self.betrayed = False\n",
    "\n",
    "        \n",
    "    def step(self):\n",
    "        if self.betrayed:\n",
    "            return Move.DEFECT\n",
    "        else:\n",
    "            return Move.COOPERATE\n",
    "        \n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        super().receive_payoff(payoff, my_move, opponent_move)\n",
    "        if not self.betrayed:\n",
    "            if opponent_move == Move.DEFECT:\n",
    "                self.betrayed = True\n",
    "        \n",
    "    def reset(self):\n",
    "        self.betrayed = False\n",
    "        \n",
    "class Pavlov(AxelrodAgent):\n",
    "    def __init__(self, unique_id, pos, model):\n",
    "        super().__init__(unique_id, pos, model)\n",
    "        self.opponent_last_move = deque([Move.COOPERATE], maxlen=1)\n",
    "        self.my_move_last_move = deque([Move.COOPERATE], maxlen=1)\n",
    "    \n",
    "    def step(self):\n",
    "        if self.opponent_last_move[0] == self.my_move_last_move[0]:\n",
    "            return self.my_move_last_move[0]\n",
    "        else:\n",
    "            if self.my_move_last_move[0] == Move.COOPERATE:\n",
    "                return Move.DEFECT\n",
    "            else:\n",
    "                return Move.COOPERATE\n",
    "    \n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        super().receive_payoff(payoff, my_move, opponent_move)\n",
    "        self.opponent_last_move.append(opponent_move)\n",
    "        self.my_move_last_move.append(opponent_move)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.opponent_last_move = deque([Move.COOPERATE], maxlen=1)\n",
    "        self.my_move_last_move = deque([Move.COOPERATE], maxlen=1)\n",
    "\n",
    "class Random(AxelrodAgent):\n",
    "    def __init__(self, unique_id, pos, model):\n",
    "        super().__init__(unique_id, pos, model)\n",
    "    \n",
    "    def step(self):\n",
    "        if self.random.getrandbits(1):\n",
    "            return Move.DEFECT\n",
    "        else:\n",
    "            return Move.COOPERATE\n",
    "        \n",
    "    def reset(self):\n",
    "        pass\n",
    "\n",
    "class NoisySpatialEvolutionaryAxelrodModel(Model):\n",
    "    #okey here we define the space in which te agents have to operate\n",
    "    #First initialize the model, giving it the parameters of noise_level, number of rounds N and the space of the space in 2D. \n",
    "    #height and width. From the superclass Model we just need the seed which is set at seed, conventiantly.\n",
    "\n",
    "    def __init__(self, N, noise_level=0.01, seed=None,\n",
    "                 height=20, width=20):\n",
    "        super().__init__(seed=seed)\n",
    "\n",
    "        #The noise level is defined to compare to the random generator later on\n",
    "        self.noise_level = noise_level\n",
    "        self.num_iterations = N\n",
    "\n",
    "        #For the agents to be stored in a sort of list is created but it is not a list it is a set. What is the difference, not much but apperaently better for the memory.\n",
    "        self.agents = set()\n",
    "\n",
    "        #Then in the model the payoff is defined. First a dictionary is created and for all 4 combinations of moves, which act as keys in the dictionairy, values are set in the forms of tulips. so tulips act as keys for tulip values in a dictionary! to be used in a set which is a sort of list. I don't really understand it either.\n",
    "\n",
    "        self.payoff_matrix = {}\n",
    "        self.payoff_matrix[(Move.COOPERATE, Move.COOPERATE)] = (2, 2)\n",
    "        self.payoff_matrix[(Move.COOPERATE, Move.DEFECT)] = (0, 3)\n",
    "        self.payoff_matrix[(Move.DEFECT, Move.COOPERATE)] = (3, 0)\n",
    "        self.payoff_matrix[(Move.DEFECT, Move.DEFECT)] = (1, 1)        \n",
    "        \n",
    "\n",
    "        #Okey now we set op the space of grid. Which is apperently also an attribute to the model class. Luckily there are predifined grids we can pick from. We choose you, SingleGrid!\n",
    "        self.grid = SingleGrid(width, height, torus=True)\n",
    "        \n",
    "        #Okey now we estbalish which strategies are available, they are stored in a list apperantly. By refereing to AxelrodAgent.__subclasses__() it will returnn a list with all the different subclasses == strategies\n",
    "        \n",
    "        strategies = AxelrodAgent.__subclasses__()\n",
    "    #Apperantly we need to know how many strategies there are so lets extrect the lenght of the list == number of strategies\n",
    "        num_strategies = len(strategies)\n",
    "    #Now we create a new attribute for this model called agent_id which apperently has to point to the number 0 for now.\n",
    "        self.agent_id = 0\n",
    "        \n",
    "    #Okey, this becomes somewhat complicated; appereently there is a attribute (hence the self.) called grid.coord.iter() This is amethod in the Singlegrid method from the MESA library. self.grid was defined earlier as SingleGrid. This method apperently iteratas over all the cells in the grid. Here 20 by 20 = 400 cells.\n",
    "   \n",
    "        for cell in self.grid.coord_iter():\n",
    "            #Each cell apperently becomes a tulip with the values, _, x, y. _ means that it is empty (for now)\n",
    "            _, x, y = cell\n",
    "            #Furthermore the posistion of each cell gets stored in the pos attribute for now. But apperantly no self. needs to be put in front of it.\n",
    "            pos = (x, y)\n",
    "            # The self.agent which was 0 gets now a 1 yeah!\n",
    "            self.agent_id += 1\n",
    "# Now we again create a variable called strategy_index to create a random integer.\n",
    "            strategy_index = int(floor(self.random.random()*num_strategies))\n",
    "# Now we create another variable called agent which uses the above variable strategy_index whole integer number to acquire from the long set of strategies a certein agent_id, posistion and self\n",
    "            agent = strategies[strategy_index](self.agent_id, pos, self)\n",
    "# This agent is then placed at the grid at the pos.\n",
    "#Okey, so we store the posistion both in the agent and in the model; which makes sense.\n",
    "            self.grid.position_agent(agent, (x, y))\n",
    "#The agent along its information of id and pos are stored in the set. which is a sort of list\n",
    "            self.agents.add(agent)\n",
    "# Now we create a new attribute which by calling a method from the MESA library calledd Datacollector. I suppose it collects data.      \n",
    "        self.datacollector = DataCollector(model_reporters={klass.__name__:klass.__name__\n",
    "                                                            for klass in strategies})\n",
    "#Since this is a for loop the cells get filled, but I dont yet know if all cells are filled? Furthermore than the ID of all agent is from 1 to 20 which is a bit dull.\n",
    "\n",
    "\n",
    "#Okey lets create a new method; count! Yes apperaently we want to count stuff.\n",
    "    def count_agent_types(self):\n",
    "        counter = Counter()\n",
    "        #For all agents stored in the self.agent set we create a dictionary with a name and how many we get from them.\n",
    "        for agent in self.agents:\n",
    "            #This means that we look at the agents in the set, we look at the class from which the agent is an instance(__class__.), then we look at the name of the class (__name_). this way we keep track of the amount of similar strategies going about in the model. \n",
    "            counter[agent.__class__.__name__] += 1\n",
    "#The counter gives to elements, since it is a dictionary; it gives the key, nameley the name here, and the amount of strategies going abour, for example 4 times the TitforTat. \n",
    "        for k,v in counter.items():\n",
    "            #The k is the key and the v is the value, 4!\n",
    "            #Setettribute does something very very special, adds attributes to a certain instance of a class.\n",
    "            #here the self is the agent. The agents has now the instance attributes of agent_id and its posistion. But we apperantly want to add to the instance also the information of its strategie name, k and the amount of similar strategies there are presetn v. I don't see the point of the letter attribute yet however.\n",
    "               setattr(self, k, v)    \n",
    "\n",
    "    def step(self):\n",
    "        #so the model calls on the stored dictionary of the counter first\n",
    "        '''Advance the model by one step.'''\n",
    "        self.count_agent_types()\n",
    "    \n",
    "        self.datacollector.collect(self)\n",
    "        #Agent_a wordt hier toegezen aan de eerste agent, met pos\n",
    "        for (agent_a, x, y) in self.grid.coord_iter():\n",
    "            #Agent_b wordt hier toegewzen met een speciale functie.\n",
    "            for agent_b in self.grid.neighbor_iter((x,y), moore=True):\n",
    "                for _ in range(self.num_iterations):\n",
    "                    #Dit blijft zo doorgaan voor het aantal iteraties gedefenieerd in deze model class.\n",
    "                    #de agent is die nu agent_a roept nu zijn eigen step functie aan. De move die eruit komt wordt toegewezen\n",
    "                    move_a = agent_a.step()\n",
    "                    move_b = agent_b.step()\n",
    "\n",
    "                    #insert noise in movement\n",
    "                    if self.random.random() < self.noise_level:\n",
    "                        if move_a == Move.COOPERATE:\n",
    "                            move_a = Move.DEFECT\n",
    "                        else:\n",
    "                            move_a = Move.COOPERATE\n",
    "                    if self.random.random() < self.noise_level:\n",
    "                        if move_b == Move.COOPERATE:\n",
    "                            move_b = Move.DEFECT\n",
    "                        else:\n",
    "                            move_b = Move.COOPERATE\n",
    "                    #de moves als tulip worden als input gebruikt voor de payoff,method die kijkt naar de dictionary met bijbehorende uikomsten tulip\n",
    "                    payoff_a, payoff_b = self.payoff_matrix[(move_a, move_b)]\n",
    "    #Voor de agenten wordt de method aangeroepen die de payoff hierboven gebruik als input en wordt opgeteld in de superclass van de agents; hier Axelrod Agent; helemaal bovenin\n",
    "                    agent_a.receive_payoff(payoff_a, move_a, move_b)\n",
    "                    agent_b.receive_payoff(payoff_b, move_b, move_a)\n",
    "                agent_a.reset()\n",
    "                agent_b.reset()\n",
    "\n",
    "        # evolution\n",
    "        # tricky, we need to determine for each grid cell\n",
    "        # is a change needed, if so, log position, agent, and type to change to\n",
    "        agents_to_change = []\n",
    "        for agent_a in self.agents:\n",
    "            neighborhood = self.grid.iter_neighbors(agent_a.pos, moore=True,\n",
    "                                                    include_center=True)\n",
    "            neighborhood = ([n for n in neighborhood])\n",
    "            neighborhood.sort(key=lambda x:x.points, reverse=True)\n",
    "            best_strategy = neighborhood[0].__class__\n",
    "            # if best type of strategy in neighborhood is\n",
    "            # different from strategy type of agent, we need\n",
    "            # to change our strategy\n",
    "            if not isinstance(agent_a, best_strategy):\n",
    "                agents_to_change.append((agent_a, best_strategy))\n",
    "\n",
    "        for entry in agents_to_change:\n",
    "            agent, klass = entry\n",
    "            self.agents.remove(agent)\n",
    "            self.grid.remove_agent(agent)\n",
    "            \n",
    "            pos = agent.pos\n",
    "            self.agent_id += 1\n",
    "            \n",
    "            new_agent = klass(self.agent_id, pos, self)\n",
    "            self.grid.position_agent(new_agent, pos)\n",
    "            self.agents.add(new_agent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In the `__init__`, we now instantiate a SingleGrid, with a specified width and height. We set the kwarg torus to True indicating we are using a donut shape grid to avoid edge effects. Next, we fill this grid with random agents of the different types. This can be implemented in various ways. What I do here is using a list with the different classes (*i.e.*, types of strategies). By drawing a random number from a unit interval, multiplying it with the lenght of the list of classes and flooring the resulting number to an integer, I now have a random index into this list with the different classes. Next, I can get the class from the list and instantiate the agent object.\n",
    "\n",
    "Some minor points with instantiating the agents. First, we give the agent a position, called pos, this is a default attribute assumed by MESA. We also still need a unique ID for the agent, we do this with a simple counter (`self.agent_id`). `self.grid.coord_iter` is a method on the grid. It returns an iterator over the cells in the grid. This iterator returns the agent occupying the cell and the x and y coordinate. Since the first item is `null` because we are filling the grid, we can ignore this. We do this by using the underscore variable name (`_`). This is a python convention. \n",
    "\n",
    "Once we have instantiated the agent, we place the agent in the grid and add it to our collection of agents. If you look in more detail at the model class, you will see that I use a set for agents, rather than a list. The reason for this is that we are going to remove agents in the evolutionary phase. Removing agents from a list is memory and compute intensive, while it is computationally easy and cheap when we have a set. \n",
    "\n",
    "```python\n",
    "self.grid = SingleGrid(width, height, torus=True)\n",
    "\n",
    "strategies = AxelrodAgent.__subclasses__()\n",
    "num_strategies = len(strategies)\n",
    "self.agent_id = 0\n",
    "\n",
    "for cell in self.grid.coord_iter():\n",
    "    _, x, y = cell\n",
    "    pos = (x, y)\n",
    "\n",
    "    self.agent_id += 1\n",
    "\n",
    "    strategy_index = int(floor(self.random.random()*num_strategies))\n",
    "\n",
    "    agent = strategies[strategy_index](self.agent_id, pos, self)\n",
    "    self.grid.position_agent(agent, (x, y))\n",
    "    self.agents.add(agent)\n",
    "```\n",
    "\n",
    "We also use a DataCollector. This is a default class provided by MESA that can be used for keeping track of relevant statistics. It can store both model level variables as well as agent level variables. Here we are only using model level variables (i.e. attributes of the model). Specifically, we are going to have an attribute on the model for each type of agent strategy (i.e. classes). This attribute is the current count of agents in the grid of the specific type. To implement this, we need to do several things.\n",
    "\n",
    "1. initialize a data collector instance\n",
    "2. at every step update the current count of agents of each strategy\n",
    "3. collect the current counts with the data collector.\n",
    "\n",
    "For step 1, we set a DataCollector as an attribute. This datacollector needs to know the names of the attributes on the model it needs to collect. So we pass a dict as kwarg to model_reporters. This dict has as key the name by which the variable will be known in the DataCollector. As value, I pass the name of the attribute on the model, but it can also be a function or method which returns a number. Note th at the ``klass`` misspelling is deliberate. The word ``class`` is protected in Python, so you cannot use it as a variable name. It is common practice to use ``klass`` instead in the rare cases were you are having variable refering to a specific class.\n",
    "\n",
    "```python\n",
    "self.datacollector = DataCollector(model_reporters={klass.__name__:klass.__name__\n",
    "                                                    for klass in strategies})\n",
    "```\n",
    "\n",
    "For step 2, we need to count at every step the number of agents per strategy type. To help keep track of this, we define a new method, `count_agent_types`. The main magic is the use of `setattr` which is a standard python function for setting attributes to a particular value on a given object. This reason for writing our code this way is that we automatically adapt our attributes to the classes of agents we have, rather than hardcoding the agent classes as attributes on our model. If we now add new classes of agents, we don't need to change the model code itself. There is also a ``getattr`` function, which is used by for example the DataCollector to get the values for the specified attribute names. \n",
    "\n",
    "```python\n",
    "def count_agent_types(self):\n",
    "    counter = Counter()\n",
    "    for agent in self.agents:\n",
    "        counter[agent.__class__.__name__] += 1\n",
    "\n",
    "    for k,v in counter.items():\n",
    "        setattr(self, k, v)    \n",
    "\n",
    "```\n",
    "\n",
    "For step 3, we modify the first part of the ``step`` method. We first count the types of agents and next collect this data with the datacollector.\n",
    "```python\n",
    "self.count_agent_types()\n",
    "self.datacollector.collect(self)\n",
    "```\n",
    "\n",
    "The remainder of the ``step`` method has also been changed quite substantially. First, We have to change against whom each agent is playing. We do this by iterating over all agents in the model. Next, we use the grid to give us the neighbors of a given agent. By setting the kwarg ``moore`` to ``True``, we indicate that we include also our diagonal neighbors. Next, we play as we did before in the noisy version of the Axelrod model.\n",
    "\n",
    "```python\n",
    "for agent_a in self.agents:\n",
    "    for agent_b in self.grid.neighbor_iter(agent_a.pos, moore=True):\n",
    "        for _ in range(self.num_iterations):\n",
    "```\n",
    "\n",
    "Second, we have to add the evolutionary dynamic. This is a bit tricky. First, we loop again over all agents in the model. We check its neighbors and see which strategy performed best. If this is of a different type (``not isinstance(agent_a, best_strategy)``, we add it to a list of agents that needs to be changed and the type of agent to which it needs to be changed. Once we know all agents that need to be changed, we can make this change. \n",
    "\n",
    "Making the change is quite straighforward. We remove the agent from the set of agents (`self.agents`) and from the grid. Next we get the position of the agent, we increment our unique ID counter, and create a new agent. This new agent is than added to the grid and to the set of agents. \n",
    "\n",
    "```python\n",
    "# evolution\n",
    "agents_to_change = []\n",
    "for agent_a in self.agents:\n",
    "    neighborhood = self.grid.iter_neighbors(agent_a.pos, moore=True,\n",
    "                                            include_center=True)\n",
    "    neighborhood = ([n for n in neighborhood])\n",
    "    neighborhood.sort(key=lambda x:x.points, reverse=True)\n",
    "    best_strategy = neighborhood[0].__class__\n",
    "    # if best type of strategy in neighborhood is\n",
    "    # different from strategy type of agent, we need\n",
    "    # to change our strategy\n",
    "    if not isinstance(agent_a, best_strategy):\n",
    "        agents_to_change.append((agent_a, best_strategy))\n",
    "\n",
    "for entry in agents_to_change:\n",
    "    agent, klass = entry\n",
    "    self.agents.remove(agent)\n",
    "    self.grid.remove_agent(agent)\n",
    "\n",
    "    pos = agent.pos\n",
    "    self.agent_id += 1\n",
    "\n",
    "    new_agent = klass(self.agent_id, pos, self)\n",
    "    self.grid.position_agent(new_agent, pos)\n",
    "    self.agents.add(new_agent)\n",
    "\n",
    "```\n",
    "\n",
    "## Assignment 1\n",
    "Can you explain why we need to first loop over all agents before we are changing a given agent to a different strategy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The first loop goes over all agents first because we need to get the data of the games; it is about the averages of a strategie not only the first round. After a first round of 1 strategie you can't decide yet. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2\n",
    "Add all agents classes (i.e., strategie) from the previous assignment to this model. Note that you might have to update the ``__init__`` method to reflect the new pos keyword argument and attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T14:11:36.301259Z",
     "start_time": "2020-10-09T14:11:36.283306Z"
    }
   },
   "outputs": [],
   "source": [
    "#__init__(self, unique_id, pos, model)\n",
    "#super().__init__(unique_id, pos, model)\n",
    "#Are changed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Assignment 3\n",
    "Run the model for 50 steps, and with 200 rounds of the iterated game. Use the defaults for all other keyword arguments.\n",
    "\n",
    "Plot the results. \n",
    "\n",
    "*hint: the DataCollector can return the statistics it has collected as a dataframe, which in turn you can plot directly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T14:14:52.510302Z",
     "start_time": "2020-10-09T14:11:50.474315Z"
    }
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'NoisySpatialEvolutionaryAxelrodModel' object has no attribute 'scores'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-8fefc23859f7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoisySpatialEvolutionaryAxelrodModel' object has no attribute 'scores'"
     ]
    }
   ],
   "source": [
    "model = NoisySpatialEvolutionaryAxelrodModel(10, 200)\n",
    "\n",
    "for _ in range(50):\n",
    "    model.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<mesa.datacollection.DataCollector object at 0x0000026C6E5AA9D0>\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'values' is not defined",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-0c5a473d3fd9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#The extraction of the agents and the values from the datacollector is too opaque for me to do. the documentation is too limited\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mDataCollector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'values' is not defined"
     ]
    }
   ],
   "source": [
    "print(DataCollector())\n",
    "fig, ax = plt.subplots()\n",
    "#The extraction of the agents and the values from the datacollector is too opaque for me to do. the documentation is too limited\n",
    "for k, v in DataCollector(values):\n",
    "    \n",
    "    ax.plot(v, label=k.__name__)\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This new model is quite a bit noisier than previously. We have a random initialization of the grid and depending on the initial neighborhood, different evolutionary dynamics can happen. On top, we have the noise in game play, and the random agent. \n",
    "\n",
    "\n",
    "## Assignment 4\n",
    "Let's explore the model for 10 replications. Run the model 10 times, with 200 rounds of the iterated prisoners dilemma. Run each model for fifty steps. Plot the results for each run. \n",
    "\n",
    "1. Can you say anything generalizable about the behavioral dynamics of the model?\n",
    "2. What do you find striking in the results and why?\n",
    "3. If you compare the results for this spatially explicit version of the Emergence of Cooperation with the none spatially explicit version, what are the most important differences in dynamics. Can you explain why adding local interactions results in these changes? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-09T14:29:38.035173Z",
     "start_time": "2020-10-09T14:19:01.045734Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.0 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "6cd8a2d2d876a48300678b662458d25d842c56bf1c18ea1e2f30c8bd47f7a1d0"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0-final"
  },
  "toc-showcode": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}