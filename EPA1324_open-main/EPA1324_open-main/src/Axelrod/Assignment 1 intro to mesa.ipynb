{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The evolution of cooperation\n",
    "\n",
    "Why do members of the same species often cooperate? Why does it seem that cooperation is evolutionarily beneficial, and how might this have come about? The economist and nobel laureate Robert Axelrod worked on these questions in the late 1970s and early 1980s. To investigate this question, he set up the following computer experiment. He investigated the performance of different strategies for playing the iterated prisoners dilemma. He set up a tournament where he invited colleagues to submit strategies. He informed them that the strategies would play the iterated prisones dillema for an unkown number of iterations. The aim was to design a strategy that would collect the most points while playing against all other submitted strategies. Some of the strategies that were submitted are\n",
    "\n",
    "* **tit for tat**; play nice, unless, in the previous move, the other player betrayed you.\n",
    "* **contrite tit for tat**; play nice, unless, in the previous two moves, the other player betrayed you.\n",
    "* **grim trigger**; play nice untill the other player betrayes you after which you will always defect as well.\n",
    "* **pavlov**; there is an entire family of pavlov strategies. The basic idea is that this strategy sticks to what was successful. The simplest version is if my and opponent move were the same last time, stay, else switch.\n",
    "* **allways defect**; \n",
    "* **allways cooperate**; \n",
    "\n",
    "More details and variations on this can be found in \n",
    "\n",
    "* [Axelrod & Hamilton (1981) The evolution of cooperations, Science, Vol. 211, No. 4489](http://www-personal.umich.edu/~axe/research/Axelrod%20and%20Hamilton%20EC%201981.pdf)\n",
    "* [Axelrod, Robert (1984), The Evolution of Cooperation, Basic Books, ISBN 0-465-02122-0](https://www.amazon.com/Evolution-Cooperation-Revised-Robert-Axelrod/dp/0465005640)\n",
    "\n",
    "In this assignment, step by step, we are going to built a model for exploring how these different strategies perform when playing the iterated prisoners dilemma. In the simplest version, we can do this using the Python anaconda distribution. However, there are also more specialized libraries for developing these models. In this course, we will be using a library called [MESA](https://mesa.readthedocs.io/en/master/). MESA, and virtually all other tools for agent based modelling rely on a particular programming paradigm known as object oriented programming. Learning about agent based modeling thus also require learning more about object oriented programming.\n",
    "\n",
    "The structure of this assignment is as follows. We start with some more background on MESA and object oriented programming. Next, we are going to apply this information by building our first agent based model of the evolution of cooperation. In this first version, a set of strategies plays the iterated prissoner dilemma game against all other strategies and we tally up the total scores. After that, we are going to slowly expand this model. First by adding some randomness to it. This is something that is very common in agent based modeling. Second, we are going to make the model dynamic by adding a small evolutionary mechanism to it. Third, we are going to combine the randomness and evolutionary dynamic. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MESA\n",
    "\n",
    "Mesa is agent-based modeling (or ABM) framework in Python. It enables users to quickly develop ABMs. It provides a variety of convenience components often used in ABMs, like different kinds of spaces within which agents can interact, different kinds of schedulers for controlling which agents in what order are making their moves, and basic support for dealing with the intrinisc stochastic nature of ABMs. MESA is ideally suited for learning agent-based modeling. It is less suited for developing large-scale computationally heavy ABMs. Given that MESA is a python library, and its focus on learning ABM, we have chosen to use MESA. The documentation of MESA can be found online: https://mesa.readthedocs.io/en/master/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is object oriented programming?\n",
    "\n",
    "There exist different programming paradigms. If you have some prior experience with more than one programming language, you might already have encountered different paradigms. Within python, it is even possible to mix and match different paradigms (up to a point) depending on your needs. Next to object oriented programming, a commonly encountered paradigm is procedural programming.\n",
    "\n",
    "In procedural programming, you describe, step by step, what should happen to solve a given task. Most programming you have been doing in Python in the previous quarter was of this style. Basically, in Python, you are using procedural programming if you use one or more functions to achieve your objectives.\n",
    "\n",
    "In contrast, in object oriented programming, you break down tasks into seperate components which have well defined behavior and state. Next, programming objectives are achieved by having these components, or objects, interact. In Python, you have been using this implicitly all the time because everything (including functions.... ) are actually objects. That is, a pandas DataFrame, for example, is actually an object. \n",
    "\n",
    "There is some terminology involved in object-oriented programming. Exact defintions of these terms are tricky and a heated topic of debate withint computer science. Below, I give loose characterizations which should be roughly right and sufficient for you to get started.\n",
    "\n",
    "* **class**; a template describing the state and behavior of a given type of objects\n",
    "* **object**; an instance of a class\n",
    "* **method**; a 'function' belonging to a class where the behavior of this 'function' is partly dependend on the state of the object (i.e. instance of the class).\n",
    "* **attribute**; a variable on a class were its value is unique to an object. Attributes are used for data that describes the state of the object and which influences the behavior of the object.\n",
    "* **inheritance**; a way of having a family of classes were some classes are subtypes of a more generic type\n",
    "\n",
    "Given that in agent-based modelling, we are interested in creating many agents and see how from their interaction aggregate patterns emerge, object-oriented programming is a natural fit. Agents are clearly objects, with state and behavior. In building models of the emergence of collaboration, you will be introduced step by step in the use of object-oriented programming in MESA (and by extension, Python).\n",
    "\n",
    "For a more thorough introduction of object oriented programming, the [Wikipedia entry](https://en.wikipedia.org/wiki/Object-oriented_programming) is quite good. For a more specific introduction of what Object-Oriented programming means in the context of python, please check: https://realpython.com/python3-object-oriented-programming/."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Developing a first model \n",
    "Below, we give the initial code you will be expanding on while developings models of the emergence of cooperation. You can look at the code block below, or scroll futher down were I explain this code in more detail.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "from enum import Enum\n",
    "from itertools import combinations\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from mesa import Model, Agent\n",
    "\n",
    "\n",
    "class Move(Enum):\n",
    "    COOPERATE = 1\n",
    "    DEFECT = 2\n",
    "\n",
    "\n",
    "class AxelrodAgent(Agent):\n",
    "    \"\"\" An agent with fixed initial wealth.\"\"\"\n",
    "    def __init__(self, unique_id, model):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.points = 0\n",
    "\n",
    "    def step(self):\n",
    "        '''\n",
    "        the move and any logic for deciding\n",
    "        on the move goes here\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        Move.COOPERATE or Move.DEFECT\n",
    "        \n",
    "        '''\n",
    "        raise NotImplemetedError\n",
    "\n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        '''\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        payoff : int\n",
    "        my_move : {Move.COOPERATE, Move.DEFECT}\n",
    "        opponements_move : {Move.COOPERATE, Move.DEFECT}\n",
    "        \n",
    "        '''\n",
    "        self.points += payoff\n",
    "        \n",
    "    def reset(self):\n",
    "        '''\n",
    "        called after playing N iterations agains\n",
    "        another player\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "        \n",
    "\n",
    "class TitForTat(AxelrodAgent):\n",
    "    \n",
    "    def __init__(self, unique_id, model):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.opponent_last_move = Move.COOPERATE\n",
    "    \n",
    "    def step(self):\n",
    "        return self.opponent_last_move\n",
    "    \n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        super().receive_payoff(payoff, my_move, opponent_move)\n",
    "        self.opponent_last_move = opponent_move\n",
    "        \n",
    "    def reset(self):\n",
    "        self.opponent_last_move = Move.COOPERATE\n",
    "\n",
    "\n",
    "class ContriteTitForTat(AxelrodAgent):\n",
    "    \n",
    "    def __init__(self, unique_id, model):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.opponent_last_two_moves = deque([Move.COOPERATE, Move.COOPERATE], maxlen=2)\n",
    "    \n",
    "    def step(self):\n",
    "        if (self.opponent_last_two_moves[0] == Move.DEFECT) and\\\n",
    "           (self.opponent_last_two_moves[1] == Move.DEFECT):\n",
    "            return Move.DEFECT\n",
    "        else:\n",
    "            return Move.COOPERATE\n",
    "    \n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        super().receive_payoff(payoff, my_move, opponent_move)\n",
    "        self.opponent_last_two_moves.append(opponent_move)\n",
    "        \n",
    "    def reset(self):\n",
    "        self.opponent_last_two_moves = deque([Move.COOPERATE, Move.COOPERATE], maxlen=2)        \n",
    "\n",
    "\n",
    "class AxelrodModel(Model):\n",
    "    \"\"\"A model with some number of agents.\"\"\"\n",
    "    def __init__(self, N, seed=None):\n",
    "        super().__init__(seed=seed)\n",
    "        self.num_iterations = N\n",
    "        self.agents = []\n",
    "        self.payoff_matrix = {}\n",
    "        \n",
    "        self.payoff_matrix[(Move.COOPERATE, Move.COOPERATE)] = (2, 2)\n",
    "        self.payoff_matrix[(Move.COOPERATE, Move.DEFECT)] = (0, 3)\n",
    "        self.payoff_matrix[(Move.DEFECT, Move.COOPERATE)] = (3, 0)\n",
    "        self.payoff_matrix[(Move.DEFECT, Move.DEFECT)] = (1, 1)\n",
    "        \n",
    "        # Create agents\n",
    "        for i, agent_class in enumerate(AxelrodAgent.__subclasses__()):\n",
    "            a = agent_class(i, self)\n",
    "            self.agents.append(a)\n",
    "\n",
    "    def step(self):\n",
    "        '''Advance the model by one step.'''\n",
    "        for agent_a, agent_b in combinations(self.agents, 2):\n",
    "            for _ in range(self.num_iterations):\n",
    "                move_a = agent_a.step()\n",
    "                move_b = agent_b.step()\n",
    "                \n",
    "                payoff_a, payoff_b = self.payoff_matrix[(move_a, move_b)]\n",
    "                \n",
    "                agent_a.receive_payoff(payoff_a, move_a, move_b)\n",
    "                agent_b.receive_payoff(payoff_b, move_b, move_a)\n",
    "            agent_a.reset()\n",
    "            agent_b.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code gives the basic setup for a first version of a model of the emergence of collaboration. Let's quickly walk through this code. \n",
    "\n",
    "We begin with a number of import statements. We import the ``deque`` class from the ``collections`` library. Deque is basically a pipeline of fixed length. We put stuff in at one end, and stuff drops of at the other end. We use the deque to create a memory of previous moves of a given length. See the ``ContriteTitForTat`` class for how we use it. Next, we import the ``Enum`` class from the ``enum`` library. Enums are a way of specifying a fixed number of unique names and associated values. We use it to standardize the 2 possible moves Agents can make. Next, we import the ``combinations`` function from the ``itertools`` library. We use this function to pair off all agents against one onther. See the ``step`` method in the ``AxelrodModel`` class. The Python programming language comes with many useful libraries. It is well worth spending some time reading the detailed documentation for in particular itertools and collections. Many common tasks can readily be accomplished by using these libraries.\n",
    "\n",
    "Next, we move to importing the ``Model`` and ``Agent`` classes from MESA. Agent is the base class from which all agents in a model have to be derived. Similarly, Model is the base class from which any given model is derived. \n",
    "\n",
    "Next, I have defined a generic ``AxelrodAgent``. Let's look at this class in a bit more detail starting with the first line\n",
    "\n",
    "```python\n",
    "class AxelrodAgent(Agent):\n",
    "```\n",
    "\n",
    "The word ``class`` is like ``def`` in that it indicates that we are describing something that can be used later. Here we are defining a class, which we can use by instantiating it as an object. We call this class ``AxelrodAgent`` and it extends (i.e. is a further detailing of) the base ``Agent`` class that we imported from Mesa.\n",
    "\n",
    "This AxelrodAgent has 4 methods\n",
    "\n",
    "```python\n",
    "    def __init__(self, unique_id, model):\n",
    "        ...\n",
    "\n",
    "    def step(self):\n",
    "        ...\n",
    "        \n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "         ...\n",
    "        \n",
    "    def reset(self):\n",
    "        ...\n",
    "\n",
    "```\n",
    "Any method in Python has as its first variable ``self``. This is not something that you need to pass when calling the method. It is something automatically inserted by Python. Self refers to this specific instance of the class and it allows you to assign values to it or call methods on itself. \n",
    "\n",
    "The first method, ``__init__`` is common to any Python class. This method is called when instantiating the class as an object. The two variables in the ``__init__``, ``unique_id`` and ``model``, are expected by MESA. The ``step`` method is also expected by Mesa. The other two methods, ``receive_payoff`` and ``reset``,have been chosen by me. Note how we are specifying, implicitly, a pattern of interaction. Each of these methods is called under specific conditions and does something to the state of the agent (receive payoff and reset), or allows the agent to behave conditional on its state (step). ``receive_payoff`` is called after each iteraction of the prisoners dilemma. ``reset`` is called after having played the iterated prisoners dilemma against another strategy.\n",
    "\n",
    "Of the 4 methods, 2 are implemented and 2 raise an error. Any specific strategy class that we are going to implement thus needs to implement alway sat least the step and reset method, while it can rely on the behavior of ``__init__`` and ``receive_payoff``, extend this behavior, or overwrite it. Let's look at these three options in some more detail.\n",
    "\n",
    "If a subclass of AxelrodAgent does not implement either ``__init__`` and ``receive_payoff``, it automatically falls back on using the behavior specified in the AxelrodAgent class. We can also extend the behavior. For this look at the ``TitForTat`` class:\n",
    "\n",
    "```python\n",
    "class TitForTat(AxelrodAgent):\n",
    "    \n",
    "    def __init__(self, unique_id, model):\n",
    "        super().__init__(unique_id, model)\n",
    "        self.opponent_last_move = Move.COOPERATE\n",
    "    \n",
    "    def step(self):\n",
    "        return self.opponent_last_move\n",
    "    \n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        super().receive_payoff(payoff, my_move, opponent_move)\n",
    "        self.opponent_last_move = opponent_move\n",
    "        \n",
    "    def reset(self):\n",
    "        self.opponent_last_move = Move.COOPERATE\n",
    "```\n",
    "\n",
    "Note how both ``__init__`` and ``receive_payoff`` start with ``super()``. This means that we first call the same method on the parent class (so ``AxelrodAgent``). Next we have some additional things we want to do. In ``__init__`` we create a novel attribute ``opponent_last_move``, which we set to ``Move.COOPERATE``. Note how we use the ``self`` variable. In receive_payoff, we update this attribute. Finally, we can overwrite the entire implementation of a method. For this, all we need to do is not call super."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: implementing your first strategies\n",
    "Before looking at the model class more closely, implement the following strategies as classes (in order of easy to difficult)\n",
    "\n",
    "* **Defector**; always defect\n",
    "* **Cooperator**; always cooperate\n",
    "* **GrimTrigger**; cooperatore untill betrayed, after which it always defects\n",
    "* **Pavlof**; The basic idea is that this strategy sticks to what was successful. The simplest version is if my and opponent move were the same last time, stay, else switch. Pavlov always starts assuming in the previous move, both agents played ``Move.COOPERATE``\n",
    "\n",
    "To help you, I have given you the basic template and all you need to do is write some code replacing the dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Defector(AxelrodAgent):\n",
    "    def step(self):\n",
    "        ...\n",
    "    \n",
    "    def reset(self):\n",
    "        ...\n",
    "    \n",
    "class Cooperator(AxelrodAgent):\n",
    "    def step(self):\n",
    "        ...\n",
    "    \n",
    "    def reset(self):\n",
    "        ...\n",
    "    \n",
    "class GrimTrigger(AxelrodAgent):\n",
    "    def __init__(self, unique_id, model):\n",
    "        super().__init__(unique_id, model)\n",
    "        ...\n",
    "        \n",
    "    def step(self):\n",
    "        ...\n",
    "        \n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        super().receive_payoff(payoff, my_move, opponent_move)\n",
    "        ...\n",
    "        \n",
    "    def reset(self):\n",
    "        ...\n",
    "        \n",
    "class Pavlov(AxelrodAgent):\n",
    "    def __init__(self, unique_id, model):\n",
    "        super().__init__(unique_id, model)\n",
    "        ...    \n",
    "    \n",
    "    def step(self):\n",
    "        ...\n",
    "    \n",
    "    def receive_payoff(self, payoff, my_move, opponent_move):\n",
    "        super().receive_payoff(payoff, my_move, opponent_move)\n",
    "        ...\n",
    "        \n",
    "    def reset(self):\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the model, let's quickly walk through the code of the model class.\n",
    "\n",
    "```python\n",
    "class AxelrodModel(Model):\n",
    "```\n",
    "\n",
    "So, here we define a new model class which extends the default model class from MESA. This class typically has at least 2 methods: ``__init__`` and ``step``. In the init we instantiate the model, while in step we specify what should happen in one step of the model. A step, or tick, is something particular to Agent Based Models. A step generally involves allowing all agents to take an action (i.e. you call step on all agents). \n",
    "\n",
    "Lets' look more closely at the init\n",
    "\n",
    "```python\n",
    "def __init__(self, N, seed=None):\n",
    "    super().__init__(seed=seed)\n",
    "    self.num_iterations = N\n",
    "    self.agents = []\n",
    "    self.payoff_matrix = {}\n",
    "\n",
    "    self.payoff_matrix[(Move.COOPERATE, Move.COOPERATE)] = (2, 2)\n",
    "    self.payoff_matrix[(Move.COOPERATE, Move.DEFECT)] = (0, 3)\n",
    "    self.payoff_matrix[(Move.DEFECT, Move.COOPERATE)] = (3, 0)\n",
    "    self.payoff_matrix[(Move.DEFECT, Move.DEFECT)] = (1, 1)\n",
    "\n",
    "    # Create agents\n",
    "    for i, agent_class in enumerate(AxelrodAgent.__subclasses__()):\n",
    "        a = agent_class(i, self)\n",
    "        self.agents.append(a)\n",
    "```\n",
    "\n",
    "we are extending the default ``__init__`` from ``Model``, so we call ``super`` first. Seed is a specific argument that will be explained in more detail later in this assignment. Next, we specify a number of attributes such as a list with all agents and the payoff matrix. Note that a high payoff is desirable. Next we populute the model with one instance of each type of strategy. \n",
    "\n",
    "```python\n",
    "for i, agent_class in enumerate(AxelrodAgent.__subclasses__()):\n",
    "```\n",
    "\n",
    "The Python buildin function ``enumerate`` takes a collection and iterates over it. It will return the index and the item itself. This allows us to loop over something while keeping track of where we are in the collection at the same time. ``AxelrodAgent.__subclasses__()`` is another python *magic* method as indicated by the leading and trailing double underscore. Moreover, this is a so-called class method. Remember, when introducing object oriented programming, I said that methods are tied to objects (i.e. instances of the class). This is 95% true, but it is possible to define methods (and attributes) at the class level as well. These are useful for doing tasks that don't relly on the state of the object but are relevant to the behavior of the class. Don't worry too much about getting your head around this. It is a rather advanced and esoteric topic that we don't need to worry about too much. Here, ``__subclasses__`` returns a list with all classes that extend ``AxelrodAgent``. That is, all the different strategies we hve been defining. Note that this also showcases a benefit of using Object Orientation. If we implement new strategies, as long as they extend ``AxelrodAgent``, we don't have to change the model itself. It will just work. \n",
    "\n",
    "Next, let's look at the step method. Basically, here we let all strategies play against all other strategies for N rounds of the prisoners dillema.\n",
    "\n",
    "```python\n",
    "def step(self):\n",
    "    '''Advance the model by one step.'''\n",
    "    for agent_a, agent_b in combinations(self.agents, 2):\n",
    "        for _ in range(self.num_iterations):\n",
    "            move_a = agent_a.step()\n",
    "            move_b = agent_b.step()\n",
    "\n",
    "            payoff_a, payoff_b = self.payoff_matrix[(move_a, move_b)]\n",
    "\n",
    "            agent_a.receive_payoff(payoff_a, move_a, move_b)\n",
    "            agent_b.receive_payoff(payoff_b, move_b, move_a)\n",
    "        agent_a.reset()\n",
    "        agent_b.reset()\n",
    "```\n",
    "\n",
    "First, we use ``combinations`` to generate all possible unique pairs of agents. Next, for each pair we play the game. We do this by first asking both agents for their move. Next, we look up the resulting payoff. Finally, we inform both agents of their payoff, their own move, and their opponents move. It might seem redundant to inform agents of their own move. And in this basic case, this is correct. But in a next version of the model, we will introduce the possiblity of error were the opposite move from the intended move is executed. Finally, after having played the game for N rounds, both agents are reset if necessary. This is to ensure that when the agents play again next, they start without any prior history based on previous games. \n",
    "\n",
    "We can now run the model and get the scores out.\n",
    "\n",
    "```python\n",
    "scores = [(agent.__class__.__name__, agent.points) for agent in model.agents]\n",
    "```\n",
    "Here we iterate over all agents, and use magic attributes to get the class name and the points acumulated over playing against all other strategies. Next, we sort this list in place based on the number of points, and sort it in reverse order.\n",
    "\n",
    "```python\n",
    "scores.sort(key=lambda x: x[1], reverse=True)\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AxelrodModel(200)\n",
    "model.step()\n",
    "\n",
    "scores = [(agent.__class__.__name__, agent.points) for agent in model.agents]\n",
    "scores.sort(key=lambda x: x[1], reverse=True)\n",
    "for entry in scores:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: adding a random agent\n",
    "\n",
    "The strategies we have been looking at so far are deterministic. Let's make this story a bit more complicated. Below, implement an aditional strategy whose moves are random with an equal chance of either cooperate or defect. How does this change the results? If you rerun this model multiple times, what do you see? Why does this happen?\n",
    "\n",
    "*tip: it is a best practice in MESA to use ``self.random`` on either any instance of a Mesa agent or model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random(AxelrodAgent):\n",
    "    \n",
    "    def step(self):\n",
    "        ...\n",
    "        \n",
    "    def reset(self):\n",
    "        pass\n",
    "    \n",
    "for _ in range(10):\n",
    "    model = AxelrodModel(200)\n",
    "    model.step()\n",
    "\n",
    "    scores = [(agent.__class__.__name__, agent.points) for agent in model.agents]\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    for entry in scores:\n",
    "        print(entry)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudo random number generation\n",
    "\n",
    "By adding an agent which plays a random move, we introduce randomness in the outcomes of the model. Every time we run the model, the payoffs received by each strategy will be slightly different. This might create all kinds of issues. For example, what if you have an error in your code that only occurs under very specific conditions. How can you ensure that these conditions occur when debugging if randomness plays a role? Or, how can we draw conclusions from results that are not deterministic?\n",
    "\n",
    "Randomness is intrinsic to virtually all agent based models. Computers don't actually produce real random numbers, but rather relly on deterministic algorithms that produce numbers that appear very close to random. Such algorithms are known as pseudo-random number generators. As long as we know the initial state of this algorithm, we can reproduce the exact same random numbers. If you want to know more, the [Wikipedia entry on Random Number Generation](https://en.wikipedia.org/wiki/Random_number_generation) is a good starting point. So how can we control this state? \n",
    "\n",
    "It is here that the ``seed`` argument comes in. Remember our model ``__init__`` function had ``seed`` as an optional keyword argument set to ``None`` by default. By providing a specific value, we can actually make the random numbers deterministic. Have a look at the code below to see this in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(123456789)\n",
    "[random.random() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123456789)\n",
    "[random.random() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting seed to the same value in both code blocks, we start the generation of random numbers from the same initial condition. Thus, the random numbers are the same. If seed is set to None, the computer will look at the time and use this as initial condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Noise\n",
    "\n",
    "In the foregoing, we have explored how well different strategies for playing the iterated prisoners dilemma perform assuming that there is no noise. That is, the moves of agents are executed as intended. Next, let's complicate the situation. What happens if there is a small chance that an agent makes a the opposite move from what it intended to do?\n",
    "\n",
    "For this, we can adapt the model itself. If you have implemented the strategies smartly, there is no need to change anything in the strategy classes. Modify the model in the following ways:\n",
    "* There is a user specifiable probability of making the opposite move. This probability is constant for all agents.\n",
    "* Both agents can simultaneously be affected by noise.\n",
    "* Agents are informed of the actual move they made.\n",
    "\n",
    "*tip: extend AxelrodModel rather than copy paste all code by adding code at the dots below*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AxelrodModelWithNoise(AxelrodModel):\n",
    "    \"\"\"A model with some number of agents.\"\"\"\n",
    "    def __init__(self, N, seed=None,noise_level=0.01):\n",
    "           ...\n",
    "    \n",
    "    def step(self):\n",
    "        '''Advance the model by one step.'''\n",
    "        for agent_a, agent_b in combinations(self.agents, 2):\n",
    "            for _ in range(self.num_iterations):\n",
    "                move_a = agent_a.step()\n",
    "                move_b = agent_b.step()\n",
    "                \n",
    "                ...\n",
    "\n",
    "                payoff_a, payoff_b = self.payoff_matrix[(move_a, move_b)]\n",
    "                \n",
    "                agent_a.receive_payoff(payoff_a, move_a, move_b)\n",
    "                agent_b.receive_payoff(payoff_b, move_b, move_a)\n",
    "            agent_a.reset()\n",
    "            agent_b.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AxelrodModelWithNoise(200)\n",
    "model.step()\n",
    "\n",
    "scores = [(agent.__class__.__name__, agent.points) for agent in model.agents]\n",
    "scores.sort(key=lambda x: x[1], reverse=True)\n",
    "for entry in scores:\n",
    "    print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with different levels of noise, ranging from 1% to 10%. How does this affect the ranking of the strategies?\n",
    "\n",
    "You can use ``np.linspace`` to generate a range of evenly spaced values between 0.01 and 0.1. You can use the above code for printing the scores of a given run to the screen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: adding evolutionary dynamics\n",
    "\n",
    "Up till now, we have run the model only for one step. That is all agents play against each other only once. Let's make the model more dynamic by adding an evolutionary component to it. We start by generating *M* agents of each strategy. These agents play against one another as before. Next, after each step, we tally up the total scores achieved by each strategy. We create a new population, proportional to how well each strategy performed. Over multiple steps, badly performing strategies will die out. However, with changing proportions of the different stategies, how well each strategy is performing will also change. Can you predict which strategies will come to dominate this population?\n",
    "\n",
    "1. Implement the ``build_population`` method which creates a population given a dictionary with proportions\n",
    "2. Calculate the new proportions as part of ``step``\n",
    "\n",
    "*hint: for keeping track at the scores per agent type in a given generation look at the ``Counter`` class in the ``collections`` library. You can get the type, or class, of an agent using the ``__class__`` attribute*\n",
    "\n",
    "To help in keeping track of the changing proportions over time, I have added a small piece of code. In the ``__init__`` we create an attribute with the scores. This attribute is a dictionariy with a list for each class of agents. In ``step`` we append the new proportions to these lists. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "class EvolutionaryAxelrodModel(Model):\n",
    "    \"\"\"A model with some number of agents.\"\"\"\n",
    "    def __init__(self, num_agents, N, seed=None):\n",
    "        super().__init__(seed=seed)\n",
    "        \n",
    "        self.num_iterations = N\n",
    "        self.agents = []\n",
    "        self.payoff_matrix = {}\n",
    "        self.population_size = len(AxelrodAgent.__subclasses__())*num_agents\n",
    "        \n",
    "        self.payoff_matrix[(Move.COOPERATE, Move.COOPERATE)] = (2, 2)\n",
    "        self.payoff_matrix[(Move.COOPERATE, Move.DEFECT)] = (0, 3)\n",
    "        self.payoff_matrix[(Move.DEFECT, Move.COOPERATE)] = (3, 0)\n",
    "        self.payoff_matrix[(Move.DEFECT, Move.DEFECT)] = (1, 1)        \n",
    "        \n",
    "        strategies = AxelrodAgent.__subclasses__()\n",
    "        num_strategies = len(strategies)\n",
    "        proportions = {agent_class:1/num_strategies for agent_class in strategies}\n",
    "\n",
    "        self.num_iterations = N\n",
    "        self.scores = defaultdict(list)\n",
    "        for agent_class in strategies:\n",
    "            self.scores[agent_class].append(proportions[agent_class])        \n",
    "        \n",
    "        self.initial_population_size = num_agents * num_strategies\n",
    "        self.agent_id = 0\n",
    "        self.build_population(proportions)\n",
    "\n",
    "    \n",
    "    def step(self):\n",
    "        '''Advance the model by one step.'''\n",
    "        for agent_a, agent_b in combinations(self.agents, 2):\n",
    "            for _ in range(self.num_iterations):\n",
    "                move_a = agent_a.step()\n",
    "                move_b = agent_b.step()\n",
    "                \n",
    "                payoff_a, payoff_b = self.payoff_matrix[(move_a, move_b)]\n",
    "                \n",
    "                agent_a.receive_payoff(payoff_a, move_a, move_b)\n",
    "                agent_b.receive_payoff(payoff_b, move_b, move_a)\n",
    "            agent_a.reset()\n",
    "            agent_b.reset()\n",
    "        \n",
    "        # calculate scores per class of agents\n",
    "        scores = Counter()\n",
    "        ...\n",
    "        \n",
    "        # normalize scores on unit interval\n",
    "        proportions = {}\n",
    "        ...\n",
    "        \n",
    "        # keep track of proportions over the generations\n",
    "        for agent_class in AxelrodAgent.__subclasses__:\n",
    "            self.scores[agent_class].append(proportions[agent_class])\n",
    "        \n",
    "        self.build_new_population(proportions)\n",
    "        \n",
    "    def build_population(self, proportions):\n",
    "        '''build the new population\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        proportions : dict\n",
    "                      key is agent class, value is float\n",
    "        \n",
    "        '''\n",
    "        \n",
    "        # build new population\n",
    "        population = []\n",
    "\n",
    "        # create a number of agents proportional to the normalized scores\n",
    "        # ensure that the total size of the population (num_agents * num_strategies) \n",
    "        # stays as close to the initial population size\n",
    "        ...\n",
    "        \n",
    "        \n",
    "        self.agents = population\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the model with 10 agents per strategy and play 200 rounds of the game. Next run the model for 100 steps and visualize how the relative proportions of the different strategies evolve over the steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EvolutionaryAxelrodModel(10, 200)\n",
    "\n",
    "for _ in range(100):\n",
    "    model.step()\n",
    "\n",
    "# visualizing results using matplotlib\n",
    "fig, ax = plt.subplots()\n",
    "for k, v in model.scores.items():\n",
    "    ax.plot(v, label=k)\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5: Evolution with noise\n",
    "\n",
    "Building on the previous two versions of the model, as a final step we are going to explore how noise affects the evolutionary dynamics. To do this, you extend the Evolutionary model from the previous step with noise. Again, explore the dynamics of this model for 100 steps over noise levels ranging from 1% to 10%. What do you see? Can you explain what is happening in the model? Are you surprized by these results?\n",
    "\n",
    "Note how in ``NoisyEvolutionaryAxelrodModel`` we only needed to slightly modify the ``__init__`` and ``step`` method from ``EvolutionaryAxelrodModel``. Due to the use of inheritance, we could reuse almost all of our code. There is still some repetition. The 2 nested for loops in ``step`` are still the same. Can you think of one or more ways to further reduce code duplication?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "\n",
    "class NoisyEvolutionaryAxelrodModel(EvolutionaryAxelrodModel):\n",
    "    \n",
    "    def __init__(self, num_agents, N, noise_level=0.01, seed=None):\n",
    "        super().__init__(num_agents, N, seed=seed)\n",
    "        self.noise_level = noise_level\n",
    "        \n",
    "    \n",
    "    def step(self):\n",
    "        '''Advance the model by one step.'''\n",
    "        for agent_a, agent_b in combinations(self.agents, 2):\n",
    "            for _ in range(self.num_iterations):\n",
    "                move_a = agent_a.step()\n",
    "                move_b = agent_b.step()\n",
    "                \n",
    "                #insert noise in movement\n",
    "                ...\n",
    "\n",
    "                payoff_a, payoff_b = self.payoff_matrix[(move_a, move_b)]\n",
    "                \n",
    "                agent_a.receive_payoff(payoff_a, move_a, move_b)\n",
    "                agent_b.receive_payoff(payoff_b, move_b, move_a)\n",
    "            agent_a.reset()\n",
    "            agent_b.reset()\n",
    "            \n",
    "        # calculate scores per class of agents\n",
    "        scores = Counter()\n",
    "        for agent in self.agents:\n",
    "            scores[agent.__class__] += agent.points\n",
    "        \n",
    "        # normalize scores on unit interval\n",
    "        total = sum(scores.values())\n",
    "        proportions = {k:v/total for k,v in scores.items()}\n",
    "        \n",
    "        # keep track of proportions over the generations\n",
    "        for agent_class in AxelrodAgent.__subclasses__():\n",
    "            self.scores[agent_class].append(proportions[agent_class])\n",
    "        \n",
    "        self.build_population(proportions)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false,
  "toc-showtags": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
